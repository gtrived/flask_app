{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I - Reddit Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As a starting step you have to write a script to collect data from ​ r/india​ .This data would be used in future parts of the problem to build the classifier. \n",
    "\n",
    "PRAW, an acronym for “Python Reddit API Wrapper”, is a python package that allows for simple access to reddit's API. PRAW aims to be as easy to use as possible and is designed to follow all of reddit's API rules.\n",
    "\n",
    "A common use for Reddit’s API is to extract comments from submissions and use them to perform keyword or phrase analysis.\n",
    "\n",
    "What all data taken : Url,Comments,Flairs,Created On,Title,Body,Number of Comments ,Score, Id,Subreddit\n",
    "How Much : 200 data per flair "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import pandas as pd\n",
    "\n",
    "db = client[\"reddit\"]\n",
    "mycol = db[\"subreddits\"]\n",
    "\n",
    "reddit = praw.Reddit(client_id='gVDCZZMu0pAOFA', client_secret='y38ARCFgtLhnSzH3Z4iwW0SvBfw', user_agent='ScrappingData')\n",
    "\n",
    "posts = []\n",
    "i = 0\n",
    "subreddit = reddit.subreddit('India')\n",
    "\n",
    "#List of all the flairs. These will be the keys in classification.\n",
    "flairs = [\"AskIndia\", \"Non-Political\", \"[R]eddiquette\", \"Scheduled\", \"Photography\", \"Science/Technology\", \"Politics\", \"Business/Finance\", \"Policy/Economy\", \"Sports\", \"Food\", \"AMA\"]\n",
    "\n",
    "i = 0\n",
    "for flair in flairs:\n",
    "    relevant_subreddits = subreddit.search(f\"flair_name:{flair}\",limit=200)\n",
    "    for submission in relevant_subreddits:\n",
    "        \n",
    "        posts.append([submission.title, submission.score, submission.id, submission.subreddit, submission.url, submission.num_comments, submission.selftext, submission.created,flair])\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        comment = ''\n",
    "        authors = ''\n",
    "        count = 0\n",
    "        for top_level_comment in submission.comments:\n",
    "            comment = comment + ' ' + top_level_comment.body\n",
    "            authors = authors + ' ' + str(top_level_comment.author)\n",
    "    \n",
    "        posts[i].append(comment)\n",
    "        i+=1\n",
    "        \n",
    "        topics_data = pd.DataFrame(posts,columns=['title','score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created','flair','comment'])\n",
    "       \n",
    "        topics_data.to_csv('FILE_trial_update_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
