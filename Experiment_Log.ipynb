{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                           # Experiment Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems - Reddit Flare Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I - Reddit Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing was to find the best way to scrap data. \n",
    "\n",
    "Methods I came across were initially :\n",
    "    \n",
    "     Beautiful Soup, scrappy ,Selinium \n",
    "\n",
    "            The advantages of Beautiful soup are —\n",
    "            1. It is easy to learn and master. for example, if we want to extract all the links from the webpage.                It can be simply done as follows —\n",
    "               \n",
    "                 from bs4 import BeautifulSoup\n",
    "                    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "                    for link in soup.find_all('a'): # It helps to find all anchor tag's\n",
    "                         print(link.get('href'))\n",
    "               In the above code, we are using the html.parser to parse the content of the html_doc. this is one                  of the strongest reason for developers to use Beautiful soup as a web scraping tool.\n",
    "\n",
    "            2. It has good comprehensive documentation which helps us to learn the things quickly.\n",
    "\n",
    "            3. It has good community support to figure out the issues that arise while we are working with this                  library.\n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links : \n",
    "    1.  https://www.jitsejan.com/using-scrapy-in-jupyter-notebook.html\n",
    "    \n",
    "    This Was the first link I hand Hands-on with.I realized scrappy is good but , I should explore some other good     tools as well.\n",
    "    \n",
    "    (Due to limited time constaints , I moved with link 2 . But I will try to implement scrapy more in future.)\n",
    "    \n",
    "    2.  https://www.privateproxyreviews.com/scrape-reddit-using-praw-python/\n",
    "        https://gilberttanner.com/blog/scraping-redditdata (how to get client ids etc are nicely mentioned here)\n",
    "    \n",
    "    This link soley deals with how to extract the reddit data, which is the requiement of part 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now As per requirement of data collection I tried by collecting 10 posts per each flare and I found I barely had data for analysis. So, I increased the count to 100 posts ( took a few hours to compute and give me a csv file).\n",
    "\n",
    "I kept that file and ran the Part 1 code again for 200 flare per post (kept my PC to compute while I went to sleep as it took a long time to compute results) to check if I get any noticeable difference while performing EDA.\n",
    "\n",
    "\n",
    "I tried to increase my dataset by using Colab for faster computation but did not got that much time as of now .(This can be done for future)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II - Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link :\n",
    "    https://chrisalbon.com/python/data_visualization/matplotlib_grouped_bar_plot/\n",
    "    https://medium.com/themlblog/splitting-csv-into-train-and-test-data-1407a063dd74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:Number of comments Vs Number of upvotes\n",
    "Using the data collected in Part I. Understanding the data collected.\n",
    "\n",
    "Number of upvotes can depict peoples interest and support towards a specific topic.Number of comments can be either in favor or can express conflict.\n",
    "\n",
    "1.If more number of comments and more number of upvotes : it will be an interesting and liked topic.\n",
    "2.If more number of comments and less number of upvotes : it can be conflicting topic \n",
    "3.If less number of comments and more number of upvotes : it will be a liked topic. \n",
    "4.If less number of comments and less number of upvotes : it will be not an interesting or liked topic\n",
    "\n",
    "Flair with maximum number of comments or upvotes can be classified as trending flair\n",
    "\n",
    "\n",
    "Example: Depending on the curent senario , Economy and Politics related comments were more in number. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and cleaning data\n",
    "\n",
    "We will be using pandas to import the dataset we will be working on and sklearn for the train_test_split() function, which will be used for splitting the data into the two parts.While working with datasets, a machine learning algorithm works in two stages — the testing and the training stage. Normally the data split between test-train is 20%-80%.\n",
    "In order to successfully implement a ML algorithm, you need to be clear about how to split the data into testing and training.\n",
    "\n",
    "Link :\n",
    "    https://medium.com/themlblog/splitting-csv-into-train-and-test-data-1407a063dd74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III - Building a Flare Detector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't really tell which algorithm will give a good result unless you implement on the data you have.\n",
    "\n",
    "A combined feature of (url+title+comments+Body) was finally used.\n",
    "\n",
    "First standalone features were tried like comments, title, body, url .etc. However the accuracy was not upto the mark. The average accuracy remained around 40-55%. After that, many features were combined on the basis of their standlone accuracies. The combination of features that gave the best accuracy (60-65%) was that of title,comments,url and body, with Logistic Regression giving the best accuracy(68.5%). After that, the training and tesing ratio was increased to improve the model (9:1), since the data was sparse.\n",
    "\n",
    "Basically six ML algorithms were used:\n",
    "\n",
    "    Naive Bayes\n",
    "    Linear Suport Vector Machine\n",
    "    Logistic Regression\n",
    "    Random Forest\n",
    "    Multi Layer Perceptron\n",
    "    Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link :\n",
    "    https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "    \n",
    "    https://docs.microsoft.com/en-us/azure/machine-learning/how-to-select-algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IV - Building a Web Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found that flask is a microservice and gets web development done .Regardless of whether your end goal is to learn Flask or Django, start with Flask. It's a great tool for learning web development fundamentals and best practices along with the core pieces of a web framework that are common to almost all frameworks. Flask is lighter and much more explicit than Django.\n",
    "\n",
    "\n",
    "Link : \n",
    "    https://www.youtube.com/watch?v=pmRT8QQLIqk ( gives a quick view on how to build and deploy a simple flask         app)\n",
    "    \n",
    "    https://www.geeksforgeeks.org/python-introduction-to-web-development-using-flask/ \n",
    "    https://www.geeksforgeeks.org/python-build-a-rest-api-using-flask/\n",
    "    \n",
    "The code of Part 1 , 2, 3  were used to perform flair detection   \n",
    "    \n",
    "    For automated-testing I found Postman a good way to test my application. Then to recheck my result I used a       script as follow\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"https://www.reddit.com/r/india/comments/g38f8m/indian_media_is_enemy_of_the_poor_antiscience/\\n\": \"['Politics']\", \"https://www.reddit.com/r/india/comments/eon5n6/should_we_have_a_caanrc_flair_like_we_had_for/\\n\": \"['Science/Technology']\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "#http://docs.python-requests.org/en/latest/user/quickstart/#post-a-multipart-encoded-file\n",
    "\n",
    "url = \"https://reddflair.herokuapp.com/automated_testing\"\n",
    "fin = open('/home/gargi/Desktop/midas/corrected/depfile/text.txt', 'rb')\n",
    "files = {'file': fin}\n",
    "try:\n",
    "  r = requests.post(url, files=files)\n",
    "  print (r.text)\n",
    "finally:\n",
    " fin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following links were useful:\n",
    "\n",
    "https://medium.com/@chaudharypulkit93/installing-postman-on-ubuntu-the-right-way-3bed244b7e51\n",
    "\n",
    "https://docs.faculty.ai/user-guide/apis/flask_apis/flask_file_upload_download.html\n",
    "\n",
    "https://stackoverflow.com/questions/49595957/restful-api-with-python-and-flask-to-request-and-read-a-text-file/49596166\n",
    "\n",
    "https://flask.palletsprojects.com/en/1.1.x/patterns/fileuploads/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part V - Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now , everything was beautifully working in my local PC. While deploying I realized its a totally different thing then working with local .\n",
    "\n",
    "Then, as soon I deployed , I was not able to see my deployed website. I ran \"heroku log --tails \" in my terminal and got application layer error : \"ImportError: No Module Named bs4 (BeautifulSoup)\"\n",
    "\n",
    "I realized a bit later which I should have paid attention at an earlier stage , the esscence of creating a virtual enviornment and working. As it gives almost the same enviornment when we deploy on heroku. \n",
    "\n",
    "When I shifted my locally working code to virtual enviornment, everything stopped working. There were versions mismatch. Pipfile was giving error and the deadline approaching was getting me anxious.\n",
    "\n",
    "I reworked my code on virtual enviornment to arrive at this link finally : https://reddflair.herokuapp.com/\n",
    "\n",
    "I still feel to look over again later , on its slow initial loading .\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
